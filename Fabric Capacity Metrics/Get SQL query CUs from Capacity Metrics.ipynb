{"cells":[{"cell_type":"markdown","source":["# <span style=\"color:red\">!! &nbsp; Important Information &nbsp; !!</span>\n","\n","<span style=\"color:red\">This code is provided as is with no guarantee of accuracy and no support. Be sure to test and validate all results in your environment.</span>"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6d2f1e61-454a-4cb9-8715-3edceacb4c50"},{"cell_type":"markdown","source":["# Notebook Parameters\n","\n","**Capacity Metrics App**\n","* capacity_metrics_workspace\n","    * The name of the workspace that hosts the Capacity Metrics app.\n","* capacity_metrics_dataset\n","    * The name of the dataset used by the Capacity Metrics app.\n","\n","**Workspace**\n","* capacity_id\n","    * The capacity which hosted the workspace where operations were run.\n","* date_of_operations\n","    * The date the operations were run.\n","* operation_id_list\n","    * The list of operations of which you want to collect the capacity unit usage. \n","    * This should be a comma separate list and each operation should be enclosed in double quotes \n","    * For example: \"AAAAAAAA-BBBB-CCCC-DDDD-1234567890AB\", \"EEEEEEEE-FFFF-GGGG-HHHH-1234567890AB\"\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ce80e491-fad4-4b04-abd6-6e776e1f97a1"},{"cell_type":"code","source":["# Capacity Metrics App\n","capacity_metrics_workspace  = 'Fabric Capacity Metrics'\n","capacity_metrics_dataset    = 'Fabric Capacity Metrics'\n","\n","# Workspace\n","capacity_id         = 'XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX'\n","date_of_operations  = 'YYYY-MM-DD'\n","operation_id_list   = \\\n","'''\n","\"AAAAAAAA-BBBB-CCCC-DDDD-1234567890AB\", \"EEEEEEEE-FFFF-GGGG-HHHH-1234567890AB\"\n","'''"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"e14f475a-6b5c-48c8-9be1-f2b04e2e6819"},{"cell_type":"markdown","source":["# Gather data from Capacity Metrics\n","Proceed with caution if making modifications to the code below.\n","<hr>"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fce19605-1469-4007-9aa2-e2a1a539fab7"},{"cell_type":"code","source":["import sempy.fabric         as fabric\n","from datetime               import datetime, timedelta\n","from pyspark.sql.functions  import col, lit, sum, min, max\n","from pyspark.sql.types      import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"00eef1a2-d375-485f-bd3c-e076f471daa2"},{"cell_type":"code","source":["def get_capacity_metrics_usage(time_point, operation_id_list):\n","\t\n","\tschema = StructType([\n","\tStructField(\"TimePoint\", \t\tStringType(), \t\tTrue),\n","\tStructField(\"WorkspaceName\", \tStringType(), \t\tTrue),\n","\tStructField(\"ItemKind\", \t\tStringType(), \t\tTrue),\n","\tStructField(\"ItemName\", \t\tStringType(), \t\tTrue),\n","\tStructField(\"StartTime\", \t\tTimestampType(), \tTrue),\n","\tStructField(\"EndTime\", \t\t\tTimestampType(), \tTrue),\n","\tStructField(\"OperationId\", \t\tStringType(), \t\tTrue),\n","\tStructField(\"Sum_CUs\", \t\t\tDoubleType(), \t\tTrue),\n","\tStructField(\"Sum_Duration\", \tIntegerType(), \t\tTrue)\n","\t])\n","\n","\tdf_empty = spark.createDataFrame([], schema)\n","\n","\tdax_command = f\"\"\"\n","\tDEFINE\n","\t\t\n","\t\tMPARAMETER 'CapacityID' \t= \"{capacity_id}\"\n","\t\tMPARAMETER 'TimePoint' \t\t= (DATE({time_point.year}, {time_point.month}, {time_point.day}) + TIME({time_point.hour}, {time_point.minute}, {time_point.second}))\n","\n","\t\tVAR __Var_CapacityId\t= {{\"{capacity_id}\"}}\n","\t\tVAR __Var_OperationId\t= {{{operation_id_list}}}\n","\n","\t\tVAR __Filter_OperationId \t= TREATAS(__Var_OperationId, 'TimePointBackgroundDetail'[OperationId])\n","\t\tVAR __Filter_CapacityId \t= TREATAS(__Var_CapacityId, 'Capacities'[capacityId])\n","\n","\t\tVAR __DS0Core = \n","\t\t\tSUMMARIZECOLUMNS(\n","\t\t\t\t'Items'[WorkspaceName],\n","\t\t\t\t'Items'[ItemKind],\n","\t\t\t\t'Items'[ItemName],\n","\t\t\t\t'TimePointBackgroundDetail'[OperationStartTime],\n","\t\t\t\t'TimePointBackgroundDetail'[OperationEndTime],\n","\t\t\t\t'TimePointBackgroundDetail'[OperationId],\n","\t\t\t\t__Filter_OperationId,\n","\t\t\t\t__Filter_CapacityId,\n","\t\t\t\t\"Sum_CUs\", CALCULATE(SUM('TimePointBackgroundDetail'[Total CU (s)])),\n","\t\t\t\t\"Sum_Duration\", CALCULATE(SUM('TimePointBackgroundDetail'[Duration (s)]))\n","\t\t\t)\n","\n","\tEVALUATE\n","\t\t__DS0Core\n","\t\"\"\"\n","\n","\tdf_dax = fabric.evaluate_dax(dax_string = dax_command, dataset = capacity_metrics_dataset, workspace = capacity_metrics_workspace)\n","\n","\tif df_dax.count()[0] > 0:\n","\t\tdf = df_empty.unionAll(\\\n","\t\t\tspark.createDataFrame(df_dax).withColumn(\"TimePoint\", lit(time_point)).select(\n","\t\t\t\tcol(\"TimePoint\"),\\\n","\t\t\t\tcol(\"Items[WorkspaceName]\").alias(\"WorkspaceName\"),\\\n","\t\t\t\tcol(\"Items[ItemKind]\").alias(\"ItemKind\"),\\\n","\t\t\t\tcol(\"Items[ItemName]\").alias(\"ItemName\"),\\\n","\t\t\t\tcol(\"TimePointBackgroundDetail[OperationStartTime]\").alias(\"StartTime\"),\\\n","\t\t\t\tcol(\"TimePointBackgroundDetail[OperationEndTime]\").alias(\"EndTime\"),\\\n","\t\t\t\tcol(\"TimePointBackgroundDetail[OperationId]\").alias(\"OperationId\"),\\\n","\t\t\t\tcol(\"[Sum_CUs]\").cast(DoubleType()).alias(\"Sum_CUs\"),\\\n","\t\t\t\tcol(\"[Sum_Duration]\").cast(IntegerType()).alias(\"Sum_Duration\"))\n","\t\t\t)\n","\telse:\n","\t\tdf = df_empty\n","\t\n","\treturn df"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"78cafea5-1b1d-4b38-8d55-7e72f6c5186e"},{"cell_type":"code","source":["'''\n","Get the capacity usage for 3PM on the date provided and 3AM on the following day.\n","This ensures that the background operations are captured no matter what time they are run as they are smoothed over a 24 hour time period.\n","Next, filter that down to the distinct records. This is necessary because a record may show up in the today and tomorrow datasets depending on the time it was run.\n","Nest, aggregate the records into a single record. This is necessary becuase some operations will have two entries, one under the executing user and one under the user \"System\".\n","Finally, display the dataframe with the clean, aggregated dataset.\n","'''\n","\n","time_point = datetime.strptime(f'{date_of_operations} 15:00:00', '%Y-%m-%d %H:%M:%S')\n","\n","df_today    = get_capacity_metrics_usage(time_point, operation_id_list)\n","df_tomorrow = get_capacity_metrics_usage(time_point + timedelta(hours = 12), operation_id_list)\n","df_all_days = df_today.unionAll(df_tomorrow)\n","df_distinct = df_all_days.select('WorkspaceName', 'ItemKind', 'ItemName', 'StartTime', 'EndTime', 'OperationId', 'Sum_CUs', 'Sum_Duration').distinct()\n","df_final    = df_distinct.groupBy('WorkspaceName', 'ItemKind', 'ItemName', 'OperationId').agg(min(\"StartTime\").alias(\"StartTime\"), max(\"EndTime\").alias(\"EndTime\"), sum(\"Sum_CUs\").alias(\"Sum_CUs\"), sum(\"Sum_Duration\").alias(\"SumDuration\"))\n","\n","display(df_final)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"aceda9ae-0885-49b6-afa6-6fb301d6028e"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"}},"nbformat":4,"nbformat_minor":5}